{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "NsCnx8w1eu32",
   "metadata": {
    "id": "NsCnx8w1eu32"
   },
   "source": [
    "### LSTM Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b47efd",
   "metadata": {
    "id": "16b47efd"
   },
   "outputs": [],
   "source": [
    "#  LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1d02a1",
   "metadata": {
    "id": "ef1d02a1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "torch.manual_seed(125)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f924588",
   "metadata": {
    "id": "4f924588"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (1.0,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89954db1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89954db1",
    "outputId": "c19b6c26-da51-43d8-e6f2-a409e672acb2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.81MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 137kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.29MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.24MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "download_root = 'MNIST_DATASET/'\n",
    "\n",
    "train_dataset = MNIST(download_root, transform=mnist_transform, train=True, download=True)\n",
    "valid_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)\n",
    "test_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f7b5e0",
   "metadata": {
    "id": "b1f7b5e0"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "valid_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "398f4215",
   "metadata": {
    "id": "398f4215"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 6000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df63cb",
   "metadata": {},
   "source": [
    "- The core ides of LSTM is to transmit information over long distances through cell states.\n",
    "- Gates control how much information is remembered and discarded:\n",
    "    - Forget Gate : Determines how much previous information to **ERASE**\n",
    "    - Input Gate : Determines how much new information to **INCORPORATE**\n",
    "    - Output Gate : Determines how much to send to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f56f3138",
   "metadata": {
    "id": "f56f3138"
   },
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        # Perform linear transformations on both the input and hidden states, outputting a vector of size : 4 * hidden_size\n",
    "        # We linearly transform the input and previous hidden states, add them, and use them in the gate computation.\n",
    "        self.x2h = nn.Linear(input_size, 4 * hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 4 * hidden_size, bias=bias)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):             # Initialize parameters with Xavier-like method\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)      # set uniform distribution and ensure initial stability\n",
    "\n",
    "    def forward(self, x, hidden):           # x : input vector of current state | hidden : tuple of (previous hidden state hx, cell state cx)\n",
    "        hx, cx = hidden\n",
    "        x = x.view(-1, x.size(1))           # x : (batch_size, input_size)\n",
    "\n",
    "        gates = self.x2h(x) + self.h2h(hx)  # (batch_size, 4*hidden_size)\n",
    "        gates = gates.squeeze()             # Remove dimension with size 1\n",
    "        # Split one vector into 4 gates / shape of each gate : (batch_size, hidden_size)\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "        ingate = F.sigmoid(ingate)\n",
    "        forgetgate = F.sigmoid(forgetgate)\n",
    "        cellgate = F.tanh(cellgate)\n",
    "        outgate = F.sigmoid(outgate)\n",
    "\n",
    "        # update cell state : cx*forgetgate for remembering + ingate*cellgate(new) for adding new infos\n",
    "        cy = torch.mul(cx, forgetgate) +  torch.mul(ingate, cellgate) \n",
    "        hy = torch.mul(outgate, F.tanh(cy))  # update new hidden state\n",
    "        return (hy, cy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71ad5c2",
   "metadata": {},
   "source": [
    "- This class is a manually implemented LSTM model that sequentially processes sequence data (e.g. sentenced, image matrices, etc.) using our own LSTMCell and produces a prediction as the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4cfc2b1",
   "metadata": {
    "id": "e4cfc2b1"
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, bias=True):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = LSTMCell(input_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x): # input x : (batch_size, seq_len, input_dim)\n",
    "        # Initial hidden state\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "        # Initial cell state \n",
    "        if torch.cuda.is_available():\n",
    "            c0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())\n",
    "        else:\n",
    "            c0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "\n",
    "        # The initial hidden and cell states for each batch are stored in hn and cn.\n",
    "        outs = [] # stores hidden state of each time step\n",
    "        cn = c0[0,:,:]\n",
    "        hn = h0[0,:,:]\n",
    "\n",
    "        '''\n",
    "        - x[:, seq, :] : The seqth time step data of the sequence (shape : (batch_size, input_dim))\n",
    "        - self.lstm(...) : Executes the previously defined LSTMCell one time step at a time.\n",
    "        - The returned hn is stored in outs[] at each time step.\n",
    "        => This iteration below processes the sequence one time step at a time, similar to the actual RNN architecture\n",
    "        '''\n",
    "        for seq in range(x.size(1)):\n",
    "            hn, cn = self.lstm(x[:,seq,:], (hn,cn)) \n",
    "            outs.append(hn)\n",
    "\n",
    "        out = outs[-1].squeeze() # outs[-1] : hidden state of the last point\n",
    "        out = self.fc(out)       # final classification \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2814ca",
   "metadata": {},
   "source": [
    "- The following setup treats MNIST input as a sequence\n",
    "- As MNIST image is 28*28, we divide it into 28 time points (sequence length), each of which is treated as a 28-dimensional vector.\n",
    "- In other words, each \"row\" is treated as a single time point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cf2c9e9",
   "metadata": {
    "id": "6cf2c9e9"
   },
   "outputs": [],
   "source": [
    "input_dim = 28    # Size of the input vector at one time point (number of pixels)\n",
    "hidden_dim = 128  # Size of the hidden state remembered by the LSTM\n",
    "layer_dim = 1     # Number of LSTM layers (only 1 layer is used)\n",
    "output_dim = 10   # Number of classification classes (digits 0-9, MNIST)\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for Multi class classification\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a6949f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7a6949f",
    "outputId": "f5fe6364-00d3-4a8e-d7f6-7a4cc94378fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 2.237457275390625. Accuracy: 21.420000076293945\n",
      "Iteration: 1000. Loss: 0.9853159785270691. Accuracy: 69.56999969482422\n",
      "Iteration: 1500. Loss: 0.41114744544029236. Accuracy: 88.91999816894531\n",
      "Iteration: 2000. Loss: 0.23155280947685242. Accuracy: 93.62000274658203\n",
      "Iteration: 2500. Loss: 0.09778112918138504. Accuracy: 94.66000366210938\n",
      "Iteration: 3000. Loss: 0.07660431414842606. Accuracy: 95.93000030517578\n",
      "Iteration: 3500. Loss: 0.12175733596086502. Accuracy: 96.41999816894531\n",
      "Iteration: 4000. Loss: 0.02374931238591671. Accuracy: 97.05999755859375\n",
      "Iteration: 4500. Loss: 0.0556069053709507. Accuracy: 96.91000366210938\n",
      "Iteration: 5000. Loss: 0.08004338294267654. Accuracy: 97.11000061035156\n",
      "Iteration: 5500. Loss: 0.16317909955978394. Accuracy: 96.5\n",
      "Iteration: 6000. Loss: 0.02393285557627678. Accuracy: 97.80000305175781\n",
      "Iteration: 6500. Loss: 0.016652461141347885. Accuracy: 97.77999877929688\n",
      "Iteration: 7000. Loss: 0.021453820168972015. Accuracy: 97.7300033569336\n",
      "Iteration: 7500. Loss: 0.026843789964914322. Accuracy: 97.88999938964844\n",
      "Iteration: 8000. Loss: 0.046674180775880814. Accuracy: 98.02999877929688\n",
      "Iteration: 8500. Loss: 0.02180766873061657. Accuracy: 97.9800033569336\n",
      "Iteration: 9000. Loss: 0.04499579221010208. Accuracy: 97.83999633789062\n"
     ]
    }
   ],
   "source": [
    "seq_dim = 28\n",
    "loss_list = [] # list to record the training loss\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.view(-1, seq_dim, input_dim).cuda()) # (batch_size, 1, 28, 28) -> (batch_size, 28, 28)\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images.view(-1, seq_dim, input_dim))\n",
    "            labels = Variable(labels)\n",
    "\n",
    "        optimizer.zero_grad()             # initialize gradient from the previous step\n",
    "        outputs = model(images)           # calculate predictions using the LSTM model\n",
    "        loss = criterion(outputs, labels) # calculate loss by comparing the predicted results to the correct answer\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            loss.cuda()\n",
    "\n",
    "        loss.backward()                # calculate gradient by backpropagation\n",
    "        optimizer.step()               # update parameter with the gradient\n",
    "        loss_list.append(loss.item())\n",
    "        iter += 1\n",
    "\n",
    "        # Validation\n",
    "        if iter % 500 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in valid_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    images = Variable(images.view(-1, seq_dim, input_dim).cuda())\n",
    "                else:\n",
    "                    images = Variable(images.view(-1 , seq_dim, input_dim))\n",
    "\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1) # The index with the highest predicted probability is considered as the class prediction\n",
    "\n",
    "                total += labels.size(0)\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6979d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    corrects, total, total_loss = 0, 0, 0\n",
    "    model.eval() # MUST!! for the evaluation mode\n",
    "    for images, labels in val_iter:\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.view(-1, seq_dim, input_dim).cuda())\n",
    "        else:\n",
    "            images = Variable(images.view(-1 , seq_dim, input_dim)).to(device)\n",
    "        labels = labels.cuda()\n",
    "        logit = model(images).cuda()\n",
    "        loss = F.cross_entropy(logit, labels, reduction = \"sum\")\n",
    "        _, predicted = torch.max(logit.data, 1) # The index with the highest predicted probability is considered as the class prediction\n",
    "        total += labels.size(0)\n",
    "        total_loss += loss.item()\n",
    "        corrects += (predicted == labels).sum()\n",
    "\n",
    "    avg_loss = total_loss / len(val_iter.dataset)\n",
    "    avg_accuracy = corrects / total\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edb78da9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edb78da9",
    "outputId": "e68eff0f-efa6-4105-95bb-9da5d9a6fa10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.06 | Test Accuracy:  0.98\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model,test_loader)\n",
    "print(\"Test Loss: %5.2f | Test Accuracy: %5.2f\" % (test_loss, test_acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
